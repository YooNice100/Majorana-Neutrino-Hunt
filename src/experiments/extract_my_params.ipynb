{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed29af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: []\n",
      "Test files: []\n",
      "Processing ../../data/MJD_Train_15.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to ../../data/params_train/MJD_Train_15_myparams.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# src/experiments/extract_my_params.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Parameter extraction helpers\n",
    "#  (If you already implemented these in src/parameters, you can replace these\n",
    "#   definitions with imports from those modules.)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def compute_tdrift99_single(wf, tp0, frac=0.999):\n",
    "    \"\"\"\n",
    "    TDrift99 in *samples*: from tp0 to first time the waveform reaches\n",
    "    `frac` of its maximum after tp0.\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    start = int(tp0)\n",
    "    if start >= n - 1:\n",
    "        return np.nan\n",
    "\n",
    "    segment = wf[start:]\n",
    "    peak_val = segment.max()\n",
    "    if peak_val <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    threshold = frac * peak_val\n",
    "    above = np.where(segment >= threshold)[0]\n",
    "    if len(above) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return float(above[0])  # in samples\n",
    "\n",
    "\n",
    "def pole_zero_correct(wf, tau_samples=500.0):\n",
    "    \"\"\"\n",
    "    Simple pole–zero correction (high-pass IIR). This doesn't have to be\n",
    "    perfect — it just needs to flatten tails reasonably for TFR.\n",
    "    \"\"\"\n",
    "    alpha = np.exp(-1.0 / tau_samples)\n",
    "    y = np.zeros_like(wf, dtype=np.float64)\n",
    "    prev_x = wf[0]\n",
    "    prev_y = 0.0\n",
    "    for i in range(1, len(wf)):\n",
    "        x = float(wf[i])\n",
    "        y_i = x - prev_x + alpha * prev_y\n",
    "        y[i] = y_i\n",
    "        prev_x = x\n",
    "        prev_y = y_i\n",
    "    return y\n",
    "\n",
    "\n",
    "def compute_tfr_single(wf, tp0, tail_offset=200, tail_len=600):\n",
    "    \"\"\"\n",
    "    Tail Flattening Ratio (TFR) = std(tail_raw) / std(tail_pz)\n",
    "    tail is taken starting at tp0 + tail_offset, for length tail_len (or to end).\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    start = int(tp0) + tail_offset\n",
    "    if start >= n - 10:  # not enough tail\n",
    "        return np.nan\n",
    "\n",
    "    end = min(n, start + tail_len)\n",
    "    tail_raw = wf[start:end].astype(np.float64)\n",
    "\n",
    "    wf_pz = pole_zero_correct(wf)\n",
    "    tail_pz = wf_pz[start:end]\n",
    "\n",
    "    std_raw = np.std(tail_raw)\n",
    "    std_pz = np.std(tail_pz)\n",
    "\n",
    "    if std_pz <= 0:\n",
    "        return np.nan\n",
    "    return float(std_raw / std_pz)\n",
    "\n",
    "\n",
    "def smooth_gaussian(x, sigma=2.0):\n",
    "    \"\"\"Simple 1D Gaussian smoothing using convolution.\"\"\"\n",
    "    if sigma <= 0:\n",
    "        return x.astype(np.float64)\n",
    "    radius = int(3 * sigma)\n",
    "    idx = np.arange(-radius, radius + 1, dtype=np.float64)\n",
    "    kernel = np.exp(-0.5 * (idx / sigma) ** 2)\n",
    "    kernel /= kernel.sum()\n",
    "    padded = np.pad(x, radius, mode=\"edge\")\n",
    "    conv = np.convolve(padded, kernel, mode=\"same\")\n",
    "    return conv[radius:-radius]\n",
    "\n",
    "\n",
    "def compute_peak_count_single(\n",
    "    wf,\n",
    "    tp0,\n",
    "    window_after_tp0=400,\n",
    "    grad_threshold_frac=0.05,\n",
    "    min_separation=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Peak Count on the gradient:\n",
    "    - baseline-subtract and normalize waveform\n",
    "    - smooth\n",
    "    - compute gradient\n",
    "    - count local maxima above grad_threshold\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    tp0 = int(tp0)\n",
    "\n",
    "    # Baseline: first 200 samples (guard against short)\n",
    "    base_end = min(200, n)\n",
    "    baseline = float(np.mean(wf[:base_end]))\n",
    "    wf_bs = wf - baseline\n",
    "\n",
    "    # Normalize by global max to make threshold comparable across events\n",
    "    max_val = np.max(np.abs(wf_bs))\n",
    "    if max_val <= 0:\n",
    "        return 0\n",
    "    wf_norm = wf_bs / max_val\n",
    "\n",
    "    # Only look near and after tp0\n",
    "    start = max(tp0 - 10, 0)\n",
    "    end = min(tp0 + window_after_tp0, n)\n",
    "    segment = wf_norm[start:end]\n",
    "\n",
    "    # Smooth and gradient\n",
    "    seg_smooth = smooth_gaussian(segment, sigma=2.0)\n",
    "    grad = np.gradient(seg_smooth)\n",
    "\n",
    "    # Threshold relative to max gradient\n",
    "    gmax = np.max(np.abs(grad))\n",
    "    if gmax <= 0:\n",
    "        return 0\n",
    "    threshold = grad_threshold_frac * gmax\n",
    "\n",
    "    # Count local maxima above threshold with minimum separation\n",
    "    count = 0\n",
    "    last_peak_idx = -min_separation - 1\n",
    "    for i in range(1, len(grad) - 1):\n",
    "        if grad[i] > grad[i - 1] and grad[i] > grad[i + 1] and grad[i] >= threshold:\n",
    "            if i - last_peak_idx >= min_separation:\n",
    "                count += 1\n",
    "                last_peak_idx = i\n",
    "\n",
    "    return int(count)\n",
    "\n",
    "\n",
    "def compute_gradient_baseline_noise_single(wf, baseline_region=(0, 200)):\n",
    "    \"\"\"\n",
    "    Gradient Baseline Noise = RMS of gradient in a pre-rise baseline window.\n",
    "    \"\"\"\n",
    "    start, end = baseline_region\n",
    "    start = max(start, 0)\n",
    "    end = min(end, len(wf))\n",
    "    if end - start < 5:\n",
    "        return np.nan\n",
    "\n",
    "    segment = wf[start:end].astype(np.float64)\n",
    "    grad = np.gradient(segment)\n",
    "    return float(np.sqrt(np.mean(grad ** 2)))\n",
    "\n",
    "\n",
    "def compute_band_power_ratio_single(\n",
    "    wf,\n",
    "    fs=100e6,\n",
    "    low_band=(0.1e6, 1e6),\n",
    "    high_band=(1e6, 10e6),\n",
    "):\n",
    "    \"\"\"\n",
    "    Band Power Ratio (BPR) = power_high / power_low using FFT of the waveform.\n",
    "    \"\"\"\n",
    "    x = wf.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "\n",
    "    # Real FFT\n",
    "    fft_vals = np.fft.rfft(x)\n",
    "    psd = np.abs(fft_vals) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(x), d=1.0 / fs)\n",
    "\n",
    "    low_mask = (freqs >= low_band[0]) & (freqs < low_band[1])\n",
    "    high_mask = (freqs >= high_band[0]) & (freqs < high_band[1])\n",
    "\n",
    "    power_low = psd[low_mask].sum()\n",
    "    power_high = psd[high_mask].sum()\n",
    "\n",
    "    if power_low <= 0:\n",
    "        return np.nan\n",
    "    return float(power_high / power_low)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Per-file processing\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def process_hdf5_file(h5_path, out_dir):\n",
    "    \"\"\"\n",
    "    Read one HDF5 file, compute all 5 parameters, and write a CSV with:\n",
    "      id, file, tdrift99, tfr, peak_count, gbn, bpr\n",
    "    \"\"\"\n",
    "    print(f\"Processing {h5_path}...\")\n",
    "    basename = os.path.basename(h5_path)\n",
    "\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        waveforms = f[\"raw_waveform\"][:]          # shape: (N, 3800)\n",
    "        tp0 = f[\"tp0\"][:]                         # shape: (N,)\n",
    "        ids = f[\"id\"][:]                          # shape: (N,)\n",
    "\n",
    "    n_events = waveforms.shape[0]\n",
    "    print(f\"  Found {n_events} waveforms.\")\n",
    "\n",
    "    tdrift_list = []\n",
    "    tfr_list = []\n",
    "    peak_count_list = []\n",
    "    gbn_list = []\n",
    "    bpr_list = []\n",
    "\n",
    "    for i in range(n_events):\n",
    "        wf = waveforms[i]\n",
    "\n",
    "        t0 = tp0[i]\n",
    "        tdrift_list.append(compute_tdrift99_single(wf, t0))\n",
    "        tfr_list.append(compute_tfr_single(wf, t0))\n",
    "        peak_count_list.append(compute_peak_count_single(wf, t0))\n",
    "        gbn_list.append(compute_gradient_baseline_noise_single(wf))\n",
    "        bpr_list.append(compute_band_power_ratio_single(wf))\n",
    "\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"    Processed {i + 1}/{n_events} events...\")\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": ids,\n",
    "            \"file\": basename,\n",
    "            \"tdrift99\": tdrift_list,\n",
    "            \"tfr\": tfr_list,\n",
    "            \"peak_count\": peak_count_list,\n",
    "            \"gbn\": gbn_list,\n",
    "            \"bpr\": bpr_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_name = os.path.splitext(basename)[0] + \"_myparams.csv\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"  Saved CSV to {out_path}\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Main: loop over all train/test files\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Notebook-safe data directory\n",
    "    DATA_DIR = os.path.abspath(\"data\")\n",
    "\n",
    "    TRAIN_PATTERN = os.path.join(DATA_DIR, \"MJD_Train*.hdf5\")\n",
    "    TEST_PATTERN = os.path.join(DATA_DIR, \"MJD_Test*.hdf5\")\n",
    "\n",
    "    OUT_DIR_TRAIN = os.path.join(DATA_DIR, \"params_train\")\n",
    "    OUT_DIR_TEST = os.path.join(DATA_DIR, \"params_test\")\n",
    "\n",
    "    train_files = sorted(glob.glob(TRAIN_PATTERN))\n",
    "    test_files = sorted(glob.glob(TEST_PATTERN))\n",
    "\n",
    "    print(\"Train files:\", train_files)\n",
    "    print(\"Test files:\", test_files)\n",
    "\n",
    "    for path in train_files:\n",
    "        process_hdf5_file(path, OUT_DIR_TRAIN)\n",
    "\n",
    "    for path in test_files:\n",
    "        process_hdf5_file(path, OUT_DIR_TEST)\n",
    "    process_hdf5_file(\"../../data/MJD_Train_15.hdf5\", \"../../data/params_train\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76cf73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MJD_Train_0.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_0_myparams.csv \n",
      "\n",
      "Processing MJD_Train_1.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_1_myparams.csv \n",
      "\n",
      "Processing MJD_Train_10.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_10_myparams.csv \n",
      "\n",
      "Processing MJD_Train_11.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_11_myparams.csv \n",
      "\n",
      "Processing MJD_Train_12.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_12_myparams.csv \n",
      "\n",
      "Processing MJD_Train_13.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_13_myparams.csv \n",
      "\n",
      "Processing MJD_Train_14.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_14_myparams.csv \n",
      "\n",
      "Processing MJD_Train_15.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_15_myparams.csv \n",
      "\n",
      "Processing MJD_Train_2.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_2_myparams.csv \n",
      "\n",
      "Processing MJD_Train_3.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_3_myparams.csv \n",
      "\n",
      "Processing MJD_Train_4.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_4_myparams.csv \n",
      "\n",
      "Processing MJD_Train_5.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_5_myparams.csv \n",
      "\n",
      "Processing MJD_Train_6.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_6_myparams.csv \n",
      "\n",
      "Processing MJD_Train_7.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_7_myparams.csv \n",
      "\n",
      "Processing MJD_Train_8.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_8_myparams.csv \n",
      "\n",
      "Processing MJD_Train_9.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_9_myparams.csv \n",
      "\n",
      "Processing MJD_Test_0.hdf5\n",
      "Saved /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_0_myparams.csv \n",
      "\n",
      "Processing MJD_Test_1.hdf5\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Unable to synchronously open file (file read failed: time = Fri Jan 23 17:10:14 2026\n, filename = '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_1.hdf5', file descriptor = 73, errno = 60, error message = 'Operation timed out', buf = 0x16d6bc760, total read size = 8, bytes this sub-read = 8, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m test_files:  process_hdf5_file(p, out_test)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    130\u001b[39m out_test  = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39m\u001b[33mparams_test\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m train_files: process_hdf5_file(p, out_train)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m test_files:  \u001b[43mprocess_hdf5_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mprocess_hdf5_file\u001b[39m\u001b[34m(h5_path, out_dir)\u001b[39m\n\u001b[32m     85\u001b[39m basename = os.path.basename(h5_path)\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing\u001b[39m\u001b[33m\"\u001b[39m, basename)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     89\u001b[39m     wfs = f[\u001b[33m\"\u001b[39m\u001b[33mraw_waveform\u001b[39m\u001b[33m\"\u001b[39m][:]\n\u001b[32m     90\u001b[39m     tp0 = f[\u001b[33m\"\u001b[39m\u001b[33mtp0\u001b[39m\u001b[33m\"\u001b[39m][:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 60] Unable to synchronously open file (file read failed: time = Fri Jan 23 17:10:14 2026\n, filename = '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_1.hdf5', file descriptor = 73, errno = 60, error message = 'Operation timed out', buf = 0x16d6bc760, total read size = 8, bytes this sub-read = 8, offset = 0)"
     ]
    }
   ],
   "source": [
    "# src/experiments/extract_my_params.py\n",
    "import os, glob, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_tdrift_single(wf, tp0, frac):\n",
    "    n = len(wf)\n",
    "    start = int(tp0)\n",
    "    if start >= n - 1:\n",
    "        return np.nan\n",
    "    seg = wf[start:]\n",
    "    peak = seg.max()\n",
    "    if peak <= 0:\n",
    "        return np.nan\n",
    "    thr = frac * peak\n",
    "    idx = np.where(seg >= thr)[0]\n",
    "    return float(idx[0]) if len(idx) else np.nan  # samples after tp0\n",
    "\n",
    "def pole_zero_correct(wf, tau_samples=500.0):\n",
    "    a = np.exp(-1.0 / tau_samples)\n",
    "    y = np.zeros_like(wf, dtype=np.float64)\n",
    "    px, py = float(wf[0]), 0.0\n",
    "    for i in range(1, len(wf)):\n",
    "        x = float(wf[i])\n",
    "        py = x - px + a * py\n",
    "        y[i] = py\n",
    "        px = x\n",
    "    return y\n",
    "\n",
    "def compute_tfr_single(wf, tp0, tail_offset=200, tail_len=600):\n",
    "    n = len(wf)\n",
    "    start = int(tp0) + tail_offset\n",
    "    if start >= n - 10:\n",
    "        return np.nan\n",
    "    end = min(n, start + tail_len)\n",
    "    tail_raw = wf[start:end].astype(np.float64)\n",
    "    tail_pz = pole_zero_correct(wf)[start:end]\n",
    "    sr, sp = np.std(tail_raw), np.std(tail_pz)\n",
    "    return np.nan if sp <= 0 else float(sr / sp)\n",
    "\n",
    "def smooth_gaussian(x, sigma=2.0):\n",
    "    if sigma <= 0:\n",
    "        return x.astype(np.float64)\n",
    "    r = int(3 * sigma)\n",
    "    kx = np.arange(-r, r + 1, dtype=np.float64)\n",
    "    k = np.exp(-0.5 * (kx / sigma) ** 2); k /= k.sum()\n",
    "    p = np.pad(x, r, mode=\"edge\")\n",
    "    return np.convolve(p, k, mode=\"same\")[r:-r]\n",
    "\n",
    "def compute_peak_count_single(wf, tp0, window_after_tp0=400, grad_thr_frac=0.05, min_sep=5):\n",
    "    n = len(wf); tp0 = int(tp0)\n",
    "    b = float(np.mean(wf[:min(200, n)]))\n",
    "    x = wf - b\n",
    "    m = np.max(np.abs(x))\n",
    "    if m <= 0: return 0\n",
    "    x = x / m\n",
    "    s, e = max(tp0 - 10, 0), min(tp0 + window_after_tp0, n)\n",
    "    seg = smooth_gaussian(x[s:e], sigma=2.0)\n",
    "    g = np.gradient(seg)\n",
    "    gmax = np.max(np.abs(g))\n",
    "    if gmax <= 0: return 0\n",
    "    thr = grad_thr_frac * gmax\n",
    "    count, last = 0, -min_sep - 1\n",
    "    for i in range(1, len(g) - 1):\n",
    "        if g[i] > g[i-1] and g[i] > g[i+1] and g[i] >= thr and (i - last) >= min_sep:\n",
    "            count += 1; last = i\n",
    "    return int(count)\n",
    "\n",
    "def compute_gbn_single(wf, baseline=(0, 200)):\n",
    "    s, e = max(baseline[0], 0), min(baseline[1], len(wf))\n",
    "    if e - s < 5: return np.nan\n",
    "    g = np.gradient(wf[s:e].astype(np.float64))\n",
    "    return float(np.sqrt(np.mean(g ** 2)))\n",
    "\n",
    "def compute_bpr_single(wf, fs=100e6, low=(0.1e6, 1e6), high=(1e6, 10e6)):\n",
    "    x = wf.astype(np.float64) - np.mean(wf)\n",
    "    F = np.fft.rfft(x)\n",
    "    psd = np.abs(F) ** 2\n",
    "    f = np.fft.rfftfreq(len(x), d=1.0 / fs)\n",
    "    pl = psd[(f >= low[0]) & (f < low[1])].sum()\n",
    "    ph = psd[(f >= high[0]) & (f < high[1])].sum()\n",
    "    return np.nan if pl <= 0 else float(ph / pl)\n",
    "\n",
    "def process_hdf5_file(h5_path, out_dir):\n",
    "    basename = os.path.basename(h5_path)\n",
    "    print(\"Processing\", basename)\n",
    "\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        wfs = f[\"raw_waveform\"][:]\n",
    "        tp0 = f[\"tp0\"][:]\n",
    "        ids = f[\"id\"][:]\n",
    "\n",
    "    t10, t50, t99, tfr, pc, gbn, bpr = ([] for _ in range(7))\n",
    "\n",
    "    for i in range(len(wfs)):\n",
    "        wf = wfs[i]; t0 = tp0[i]\n",
    "        t10.append(compute_tdrift_single(wf, t0, 0.10))\n",
    "        t50.append(compute_tdrift_single(wf, t0, 0.50))\n",
    "        t99.append(compute_tdrift_single(wf, t0, 0.999))\n",
    "        tfr.append(compute_tfr_single(wf, t0))\n",
    "        pc.append(compute_peak_count_single(wf, t0))\n",
    "        gbn.append(compute_gbn_single(wf))\n",
    "        bpr.append(compute_bpr_single(wf))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"file\": basename,\n",
    "        \"tdrift10\": t10,\n",
    "        \"tdrift50\": t50,\n",
    "        \"tdrift99\": t99,\n",
    "        \"tfr\": tfr,\n",
    "        \"peak_count\": pc,\n",
    "        \"gbn\": gbn,\n",
    "        \"bpr\": bpr,\n",
    "    })\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, os.path.splitext(basename)[0] + \"_myparams.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved\", out_path, \"\\n\")\n",
    "\n",
    "def main():\n",
    "    REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "    DATA_DIR = os.path.join(REPO_ROOT, \"data\")\n",
    "\n",
    "    train_files = sorted(glob.glob(os.path.join(DATA_DIR, \"MJD_Train*.hdf5\")))\n",
    "    test_files  = sorted(glob.glob(os.path.join(DATA_DIR, \"MJD_Test*.hdf5\")))\n",
    "\n",
    "    out_train = os.path.join(DATA_DIR, \"params_train\")\n",
    "    out_test  = os.path.join(DATA_DIR, \"params_test\")\n",
    "\n",
    "    for p in train_files: process_hdf5_file(p, out_train)\n",
    "    for p in test_files:  process_hdf5_file(p, out_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aa8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD =\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344066c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa0c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments\n",
      "data exists? False\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCWD =\u001b[39m\u001b[33m\"\u001b[39m, os.getcwd())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdata exists?\u001b[39m\u001b[33m\"\u001b[39m, os.path.exists(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfiles in data:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD =\", os.getcwd())\n",
    "print(\"data exists?\", os.path.exists(\"data\"))\n",
    "print(\"files in data:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181ecc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = (Path.cwd() / \"data\").resolve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bf96f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_PATTERN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTRAIN_PATTERN =\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mTRAIN_PATTERN\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMatches:\u001b[39m\u001b[33m\"\u001b[39m, glob.glob(TRAIN_PATTERN))\n",
      "\u001b[31mNameError\u001b[39m: name 'TRAIN_PATTERN' is not defined"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "print(\"TRAIN_PATTERN =\", TRAIN_PATTERN)\n",
    "print(\"Matches:\", glob.glob(TRAIN_PATTERN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419403a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments\n",
      "DATA_DIR: /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments/data\n",
      "Train files found: []\n",
      "Test files found: []\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "DATA_DIR = (Path.cwd() / \"data\").resolve()   # assumes you opened VSCode at repo root\n",
    "TRAIN_PATTERN = str(DATA_DIR / \"MJD_Train*.hdf5\")\n",
    "TEST_PATTERN  = str(DATA_DIR / \"MJD_Test*.hdf5\")\n",
    "\n",
    "train_files = sorted(glob.glob(TRAIN_PATTERN))\n",
    "test_files  = sorted(glob.glob(TEST_PATTERN))\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Train files found:\", train_files)\n",
    "print(\"Test files found:\", test_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25478b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'data/MJD_Test_1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mh5py\u001b[39;00m\n\u001b[32m      2\u001b[39m p = \u001b[33m\"\u001b[39m\u001b[33mdata/MJD_Test_1.hdf5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(f.keys()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'data/MJD_Test_1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "p = \"data/MJD_Test_1.hdf5\"\n",
    "with h5py.File(p, \"r\") as f:\n",
    "    print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998f5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['detector', 'energy_label', 'id', 'psd_label_dcr', 'psd_label_high_avse', 'psd_label_low_avse', 'psd_label_lq', 'raw_waveform', 'run_number', 'tp0']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "p = \"../../data/MJD_Test_1.hdf5\"\n",
    "with h5py.File(p, \"r\") as f:\n",
    "    print(list(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414070f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: ['/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_0.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_1.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_10.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_11.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_12.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_13.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_14.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_15.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_2.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_3.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_4.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_5.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_6.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_7.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_8.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_9.hdf5']\n",
      "Test files: ['/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_0.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_1.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_2.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_3.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_4.hdf5', '/Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_5.hdf5']\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_0.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_0_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_1.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_1_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_10.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_10_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_11.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_11_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_12.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_12_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_13.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_13_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_14.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_14_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_15.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_15_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_2.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_2_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_3.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_3_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_4.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_4_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_5.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_5_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_6.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_6_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_7.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_7_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_8.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_8_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Train_9.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_train/MJD_Train_9_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_0.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_0_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_1.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_1_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_2.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_2_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_3.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_3_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_4.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_4_myparams.csv\n",
      "\n",
      "Processing /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/MJD_Test_5.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test/MJD_Test_5_myparams.csv\n",
      "\n",
      "Processing ../../data/MJD_Train_15.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to ../../data/params_train/MJD_Train_15_myparams.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# src/experiments/extract_my_params.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Parameter extraction helpers\n",
    "#  (If you already implemented these in src/parameters, you can replace these\n",
    "#   definitions with imports from those modules.)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def compute_tdrift99_single(wf, tp0, frac=0.999):\n",
    "    \"\"\"\n",
    "    TDrift99 in *samples*: from tp0 to first time the waveform reaches\n",
    "    `frac` of its maximum after tp0.\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    start = int(tp0)\n",
    "    if start >= n - 1:\n",
    "        return np.nan\n",
    "\n",
    "    segment = wf[start:]\n",
    "    peak_val = segment.max()\n",
    "    if peak_val <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    threshold = frac * peak_val\n",
    "    above = np.where(segment >= threshold)[0]\n",
    "    if len(above) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return float(above[0])  # in samples\n",
    "\n",
    "\n",
    "def pole_zero_correct(wf, tau_samples=500.0):\n",
    "    \"\"\"\n",
    "    Simple pole–zero correction (high-pass IIR). This doesn't have to be\n",
    "    perfect — it just needs to flatten tails reasonably for TFR.\n",
    "    \"\"\"\n",
    "    alpha = np.exp(-1.0 / tau_samples)\n",
    "    y = np.zeros_like(wf, dtype=np.float64)\n",
    "    prev_x = wf[0]\n",
    "    prev_y = 0.0\n",
    "    for i in range(1, len(wf)):\n",
    "        x = float(wf[i])\n",
    "        y_i = x - prev_x + alpha * prev_y\n",
    "        y[i] = y_i\n",
    "        prev_x = x\n",
    "        prev_y = y_i\n",
    "    return y\n",
    "\n",
    "\n",
    "def compute_tfr_single(wf, tp0, tail_offset=200, tail_len=600):\n",
    "    \"\"\"\n",
    "    Tail Flattening Ratio (TFR) = std(tail_raw) / std(tail_pz)\n",
    "    tail is taken starting at tp0 + tail_offset, for length tail_len (or to end).\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    start = int(tp0) + tail_offset\n",
    "    if start >= n - 10:  # not enough tail\n",
    "        return np.nan\n",
    "\n",
    "    end = min(n, start + tail_len)\n",
    "    tail_raw = wf[start:end].astype(np.float64)\n",
    "\n",
    "    wf_pz = pole_zero_correct(wf)\n",
    "    tail_pz = wf_pz[start:end]\n",
    "\n",
    "    std_raw = np.std(tail_raw)\n",
    "    std_pz = np.std(tail_pz)\n",
    "\n",
    "    if std_pz <= 0:\n",
    "        return np.nan\n",
    "    return float(std_raw / std_pz)\n",
    "\n",
    "\n",
    "def smooth_gaussian(x, sigma=2.0):\n",
    "    \"\"\"Simple 1D Gaussian smoothing using convolution.\"\"\"\n",
    "    if sigma <= 0:\n",
    "        return x.astype(np.float64)\n",
    "    radius = int(3 * sigma)\n",
    "    idx = np.arange(-radius, radius + 1, dtype=np.float64)\n",
    "    kernel = np.exp(-0.5 * (idx / sigma) ** 2)\n",
    "    kernel /= kernel.sum()\n",
    "    padded = np.pad(x, radius, mode=\"edge\")\n",
    "    conv = np.convolve(padded, kernel, mode=\"same\")\n",
    "    return conv[radius:-radius]\n",
    "\n",
    "\n",
    "def compute_peak_count_single(\n",
    "    wf,\n",
    "    tp0,\n",
    "    window_after_tp0=400,\n",
    "    grad_threshold_frac=0.05,\n",
    "    min_separation=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Peak Count on the gradient:\n",
    "    - baseline-subtract and normalize waveform\n",
    "    - smooth\n",
    "    - compute gradient\n",
    "    - count local maxima above grad_threshold\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    tp0 = int(tp0)\n",
    "\n",
    "    # Baseline: first 200 samples (guard against short)\n",
    "    base_end = min(200, n)\n",
    "    baseline = float(np.mean(wf[:base_end]))\n",
    "    wf_bs = wf - baseline\n",
    "\n",
    "    # Normalize by global max to make threshold comparable across events\n",
    "    max_val = np.max(np.abs(wf_bs))\n",
    "    if max_val <= 0:\n",
    "        return 0\n",
    "    wf_norm = wf_bs / max_val\n",
    "\n",
    "    # Only look near and after tp0\n",
    "    start = max(tp0 - 10, 0)\n",
    "    end = min(tp0 + window_after_tp0, n)\n",
    "    segment = wf_norm[start:end]\n",
    "\n",
    "    # Smooth and gradient\n",
    "    seg_smooth = smooth_gaussian(segment, sigma=2.0)\n",
    "    grad = np.gradient(seg_smooth)\n",
    "\n",
    "    # Threshold relative to max gradient\n",
    "    gmax = np.max(np.abs(grad))\n",
    "    if gmax <= 0:\n",
    "        return 0\n",
    "    threshold = grad_threshold_frac * gmax\n",
    "\n",
    "    # Count local maxima above threshold with minimum separation\n",
    "    count = 0\n",
    "    last_peak_idx = -min_separation - 1\n",
    "    for i in range(1, len(grad) - 1):\n",
    "        if grad[i] > grad[i - 1] and grad[i] > grad[i + 1] and grad[i] >= threshold:\n",
    "            if i - last_peak_idx >= min_separation:\n",
    "                count += 1\n",
    "                last_peak_idx = i\n",
    "\n",
    "    return int(count)\n",
    "\n",
    "\n",
    "def compute_gradient_baseline_noise_single(wf, baseline_region=(0, 200)):\n",
    "    \"\"\"\n",
    "    Gradient Baseline Noise = RMS of gradient in a pre-rise baseline window.\n",
    "    \"\"\"\n",
    "    start, end = baseline_region\n",
    "    start = max(start, 0)\n",
    "    end = min(end, len(wf))\n",
    "    if end - start < 5:\n",
    "        return np.nan\n",
    "\n",
    "    segment = wf[start:end].astype(np.float64)\n",
    "    grad = np.gradient(segment)\n",
    "    return float(np.sqrt(np.mean(grad ** 2)))\n",
    "\n",
    "\n",
    "def compute_band_power_ratio_single(\n",
    "    wf,\n",
    "    fs=100e6,\n",
    "    low_band=(0.1e6, 1e6),\n",
    "    high_band=(1e6, 10e6),\n",
    "):\n",
    "    \"\"\"\n",
    "    Band Power Ratio (BPR) = power_high / power_low using FFT of the waveform.\n",
    "    \"\"\"\n",
    "    x = wf.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "\n",
    "    # Real FFT\n",
    "    fft_vals = np.fft.rfft(x)\n",
    "    psd = np.abs(fft_vals) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(x), d=1.0 / fs)\n",
    "\n",
    "    low_mask = (freqs >= low_band[0]) & (freqs < low_band[1])\n",
    "    high_mask = (freqs >= high_band[0]) & (freqs < high_band[1])\n",
    "\n",
    "    power_low = psd[low_mask].sum()\n",
    "    power_high = psd[high_mask].sum()\n",
    "\n",
    "    if power_low <= 0:\n",
    "        return np.nan\n",
    "    return float(power_high / power_low)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Per-file processing\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def process_hdf5_file(h5_path, out_dir):\n",
    "    \"\"\"\n",
    "    Read one HDF5 file, compute all parameters, and write a CSV with:\n",
    "      id, file, tdrift10, tdrift50, tdrift99, tfr, peak_count, gbn, bpr\n",
    "    \"\"\"\n",
    "    print(f\"Processing {h5_path}...\")\n",
    "    basename = os.path.basename(h5_path)\n",
    "\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        waveforms = f[\"raw_waveform\"][:]          # shape: (N, 3800)\n",
    "        tp0 = f[\"tp0\"][:]                         # shape: (N,)\n",
    "        ids = f[\"id\"][:]                          # shape: (N,)\n",
    "\n",
    "    n_events = waveforms.shape[0]\n",
    "    print(f\"  Found {n_events} waveforms.\")\n",
    "\n",
    "    # --- ADDED: tdrift10 + tdrift50 lists (original lists preserved) ---\n",
    "    tdrift10_list = []\n",
    "    tdrift50_list = []\n",
    "\n",
    "    tdrift_list = []\n",
    "    tfr_list = []\n",
    "    peak_count_list = []\n",
    "    gbn_list = []\n",
    "    bpr_list = []\n",
    "\n",
    "    for i in range(n_events):\n",
    "        wf = waveforms[i]\n",
    "        t0 = tp0[i]\n",
    "\n",
    "        # --- ADDED: compute tdrift10 + tdrift50 using your existing function ---\n",
    "        tdrift10_list.append(compute_tdrift99_single(wf, t0, frac=0.10))\n",
    "        tdrift50_list.append(compute_tdrift99_single(wf, t0, frac=0.50))\n",
    "\n",
    "        # existing tdrift99 behavior preserved\n",
    "        tdrift_list.append(compute_tdrift99_single(wf, t0))\n",
    "\n",
    "        tfr_list.append(compute_tfr_single(wf, t0))\n",
    "        peak_count_list.append(compute_peak_count_single(wf, t0))\n",
    "        gbn_list.append(compute_gradient_baseline_noise_single(wf))\n",
    "        bpr_list.append(compute_band_power_ratio_single(wf))\n",
    "\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"    Processed {i + 1}/{n_events} events...\")\n",
    "\n",
    "    # --- ADDED: columns tdrift10 + tdrift50 (everything else unchanged) ---\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": ids,\n",
    "            \"file\": basename,\n",
    "            \"tdrift10\": tdrift10_list,\n",
    "            \"tdrift50\": tdrift50_list,\n",
    "            \"tdrift99\": tdrift_list,\n",
    "            \"tfr\": tfr_list,\n",
    "            \"peak_count\": peak_count_list,\n",
    "            \"gbn\": gbn_list,\n",
    "            \"bpr\": bpr_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_name = os.path.splitext(basename)[0] + \"_myparams.csv\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"  Saved CSV to {out_path}\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Main: loop over all train/test files\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Notebook-safe data directory\n",
    "    DATA_DIR = os.path.abspath(\"../../data\")\n",
    "\n",
    "    TRAIN_PATTERN = os.path.join(DATA_DIR, \"MJD_Train*.hdf5\")\n",
    "    TEST_PATTERN = os.path.join(DATA_DIR, \"MJD_Test*.hdf5\")\n",
    "\n",
    "    OUT_DIR_TRAIN = os.path.join(DATA_DIR, \"params_train\")\n",
    "    OUT_DIR_TEST = os.path.join(DATA_DIR, \"params_test\")\n",
    "\n",
    "    train_files = sorted(glob.glob(TRAIN_PATTERN))\n",
    "    test_files = sorted(glob.glob(TEST_PATTERN))\n",
    "\n",
    "    print(\"Train files:\", train_files)\n",
    "    print(\"Test files:\", test_files)\n",
    "\n",
    "    for path in train_files:\n",
    "        process_hdf5_file(path, OUT_DIR_TRAIN)\n",
    "\n",
    "    for path in test_files:\n",
    "        process_hdf5_file(path, OUT_DIR_TEST)\n",
    "\n",
    "    process_hdf5_file(\"../../data/MJD_Train_15.hdf5\", \"../../data/params_train\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9921ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD =\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651cc5fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     pd.concat(frames, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m).to_csv(out_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, out_path)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mmerge_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_2.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m merge_split(TEST_DIR,  \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,  OUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mtest_2.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mmerge_split\u001b[39m\u001b[34m(split_dir, split_label, out_path)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     22\u001b[39m     fileno = file_no_from_name(f)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m\u001b[33m column\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 60] Operation timed out"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "TRAIN_DIR = Path(\"../../data/params_train\")\n",
    "TEST_DIR  = Path(\"../../data/params_test\")\n",
    "\n",
    "OUT_DIR = Path(\"../../extracted_features_csv_files/prithvi_csv_files\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def file_no_from_name(p: Path) -> int:\n",
    "    m = re.search(r\"_(\\d+)\", p.stem)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "def merge_split(split_dir: Path, split_label: str, out_path: Path):\n",
    "    files = sorted(split_dir.glob(\"*.csv\"), key=file_no_from_name)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {split_dir.resolve()}\")\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        fileno = file_no_from_name(f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        if \"id\" not in df.columns:\n",
    "            raise ValueError(f\"{f.name} missing 'id' column\")\n",
    "\n",
    "        df[\"id\"] = df[\"id\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        df[\"id\"] = df[\"id\"] + f\"_{split_label}_{fileno}\"\n",
    "\n",
    "        for col in [\"file\", \"filename\"]:\n",
    "            if col in df.columns:\n",
    "                df = df.drop(columns=[col])\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    pd.concat(frames, ignore_index=True).to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "merge_split(TRAIN_DIR, \"train\", OUT_DIR / \"train_2.csv\")\n",
    "merge_split(TEST_DIR,  \"test\",  OUT_DIR / \"test_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d0891f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     out_df.to_csv(out_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cols=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out_df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mmerge_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_2.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m merge_split(TEST_DIR,  \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,  OUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mtest_2.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mmerge_split\u001b[39m\u001b[34m(split_dir, split_label, out_path)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     25\u001b[39m     fileno = file_no_from_name(f)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# --- Build new id using existing df['id'] ---\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 60] Operation timed out"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- CHANGE THESE PATHS TO WHERE YOUR PER-FILE CSVs ACTUALLY LIVE ---\n",
    "TRAIN_DIR = Path(\"../../data/params_train\")\n",
    "TEST_DIR  = Path(\"../../data/params_test\")\n",
    "\n",
    "OUT_DIR = Path(\"../../extracted_features_csv_files/prithvi_csv_files\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def file_no_from_name(path: Path) -> int:\n",
    "    # works for names like: MJD_Train_0_myparams.csv OR Train_0.csv OR *_0_*.csv\n",
    "    m = re.search(r\"_(\\d+)\", path.stem)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "def merge_split(split_dir: Path, split_label: str, out_path: Path):\n",
    "    files = sorted(split_dir.glob(\"*.csv\"), key=file_no_from_name)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {split_dir.resolve()}\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for f in files:\n",
    "        fileno = file_no_from_name(f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # --- Build new id using existing df['id'] ---\n",
    "        if \"id\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"File {f.name} is missing an 'id' column. \"\n",
    "                \"If some files don't have id, tell me and I'll add a fallback.\"\n",
    "            )\n",
    "\n",
    "        # Make sure id is clean + string\n",
    "        original_id = df[\"id\"].astype(str).str.strip()\n",
    "\n",
    "        # Optional: if some ids look like '123.0' because of CSV typing, normalize them\n",
    "        # (keeps '123' instead of '123.0' when it's really an integer)\n",
    "        original_id = original_id.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "        # New id format: <original_id>_<train|test>_<fileno>\n",
    "        df[\"id\"] = original_id + f\"_{split_label}_{fileno}\"\n",
    "\n",
    "        # Remove other per-file identifiers if they exist (but keep the new 'id')\n",
    "        for col in [\"file\", \"filename\"]:\n",
    "            if col in df.columns:\n",
    "                df = df.drop(columns=[col])\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    out_df = pd.concat(frames, ignore_index=True)\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {split_label} -> {out_path} | rows={len(out_df)} cols={len(out_df.columns)}\")\n",
    "\n",
    "merge_split(TRAIN_DIR, \"train\", OUT_DIR / \"train_2.csv\")\n",
    "merge_split(TEST_DIR,  \"test\",  OUT_DIR / \"test_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51388d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../../extracted_features_csv_files/prithvi_csv_files/train_2.csv\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, out_path)\n\u001b[32m     43\u001b[39m merge_split_chunked(TRAIN_DIR, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, OUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mtrain_2.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mmerge_split_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest_2.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mmerge_split_chunked\u001b[39m\u001b[34m(split_dir, split_label, out_path, chunksize)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     21\u001b[39m     fileno = file_no_from_name(f)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m chunk.columns:\n\u001b[32m     25\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m\u001b[33m column\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 60] Operation timed out"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "TRAIN_DIR = Path(\"../../data/params_train\")\n",
    "TEST_DIR  = Path(\"../../data/params_test\")\n",
    "OUT_DIR   = Path(\"../../extracted_features_csv_files/prithvi_csv_files\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def file_no_from_name(p: Path) -> int:\n",
    "    m = re.search(r\"_(\\d+)\", p.stem)  # grabs _0, _1, etc.\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "def merge_split_chunked(split_dir: Path, split_label: str, out_path: Path, chunksize: int = 50_000):\n",
    "    files = sorted(split_dir.glob(\"*.csv\"), key=file_no_from_name)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {split_dir.resolve()}\")\n",
    "\n",
    "    first_write = True\n",
    "    for f in files:\n",
    "        fileno = file_no_from_name(f)\n",
    "\n",
    "        for chunk in pd.read_csv(f, chunksize=chunksize):\n",
    "            if \"id\" not in chunk.columns:\n",
    "                raise ValueError(f\"{f.name} missing 'id' column\")\n",
    "\n",
    "            # build id: <original_id>_<train|test>_<fileno>\n",
    "            chunk[\"id\"] = (\n",
    "                chunk[\"id\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "                + f\"_{split_label}_{fileno}\"\n",
    "            )\n",
    "\n",
    "            # drop per-file identifiers if present\n",
    "            for col in [\"file\", \"filename\"]:\n",
    "                if col in chunk.columns:\n",
    "                    chunk = chunk.drop(columns=[col])\n",
    "\n",
    "            chunk.to_csv(out_path, index=False, mode=\"w\" if first_write else \"a\", header=first_write)\n",
    "            first_write = False\n",
    "\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "merge_split_chunked(TRAIN_DIR, \"train\", OUT_DIR / \"train_2.csv\")\n",
    "merge_split_chunked(TEST_DIR,  \"test\",  OUT_DIR / \"test_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f00470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: MJD_Test_0_myparams.csv\n",
      "OK: MJD_Test_1_myparams.csv\n",
      "OK: MJD_Test_2_myparams.csv\n",
      "OK: MJD_Test_3_myparams.csv\n",
      "OK: MJD_Test_4_myparams.csv\n",
      "OK: MJD_Test_5_myparams.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "TEST_DIR = Path(\"../../data/params_test\")\n",
    "\n",
    "def file_no_from_name(p: Path) -> int:\n",
    "    m = re.search(r\"_(\\d+)\", p.stem)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "files = sorted(TEST_DIR.glob(\"*.csv\"), key=file_no_from_name)\n",
    "\n",
    "for f in files:\n",
    "    try:\n",
    "        pd.read_csv(f, nrows=5)\n",
    "        print(\"OK:\", f.name)\n",
    "    except Exception as e:\n",
    "        print(\"FAIL:\", f.name, \"|\", type(e).__name__, e)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f47a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to: /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/data/params_test_local\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "SRC = Path(\"../../data/params_test\")\n",
    "DST = Path(\"../../data/params_test_local\")\n",
    "DST.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for f in SRC.glob(\"*.csv\"):\n",
    "    shutil.copy2(f, DST / f.name)\n",
    "\n",
    "print(\"Copied to:\", DST.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f250bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../../extracted_features_csv_files/prithvi_csv_files/test_2.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "TEST_DIR = Path(\"../../data/params_test_local\")\n",
    "OUT_DIR  = Path(\"../../extracted_features_csv_files/prithvi_csv_files\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def file_no_from_name(p: Path) -> int:\n",
    "    m = re.search(r\"_(\\d+)\", p.stem)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "out_path = OUT_DIR / \"test_2.csv\"\n",
    "first = True\n",
    "\n",
    "for f in sorted(TEST_DIR.glob(\"*.csv\"), key=file_no_from_name):\n",
    "    fileno = file_no_from_name(f)\n",
    "    for chunk in pd.read_csv(f, chunksize=20_000):  # smaller = safer\n",
    "        chunk[\"id\"] = chunk[\"id\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True) + f\"_test_{fileno}\"\n",
    "        for col in [\"file\", \"filename\"]:\n",
    "            if col in chunk.columns:\n",
    "                chunk = chunk.drop(columns=[col])\n",
    "        chunk.to_csv(out_path, index=False, mode=\"w\" if first else \"a\", header=first)\n",
    "        first = False\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f2923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
