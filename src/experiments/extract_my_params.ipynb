{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed29af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: []\n",
      "Test files: []\n",
      "Processing ../../data/MJD_Train_15.hdf5...\n",
      "  Found 65000 waveforms.\n",
      "    Processed 5000/65000 events...\n",
      "    Processed 10000/65000 events...\n",
      "    Processed 15000/65000 events...\n",
      "    Processed 20000/65000 events...\n",
      "    Processed 25000/65000 events...\n",
      "    Processed 30000/65000 events...\n",
      "    Processed 35000/65000 events...\n",
      "    Processed 40000/65000 events...\n",
      "    Processed 45000/65000 events...\n",
      "    Processed 50000/65000 events...\n",
      "    Processed 55000/65000 events...\n",
      "    Processed 60000/65000 events...\n",
      "    Processed 65000/65000 events...\n",
      "  Saved CSV to ../../data/params_train/MJD_Train_15_myparams.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# src/experiments/extract_my_params.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Parameter extraction helpers\n",
    "#  (If you already implemented these in src/parameters, you can replace these\n",
    "#   definitions with imports from those modules.)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def compute_tdrift99_single(wf, tp0, frac=0.999):\n",
    "    \"\"\"\n",
    "    TDrift99 in *samples*: from tp0 to first time the waveform reaches\n",
    "    `frac` of its maximum after tp0.\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    start = int(tp0)\n",
    "    if start >= n - 1:\n",
    "        return np.nan\n",
    "\n",
    "    segment = wf[start:]\n",
    "    peak_val = segment.max()\n",
    "    if peak_val <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    threshold = frac * peak_val\n",
    "    above = np.where(segment >= threshold)[0]\n",
    "    if len(above) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return float(above[0])  # in samples\n",
    "\n",
    "\n",
    "def pole_zero_correct(wf, tau_samples=500.0):\n",
    "    \"\"\"\n",
    "    Simple pole–zero correction (high-pass IIR). This doesn't have to be\n",
    "    perfect — it just needs to flatten tails reasonably for TFR.\n",
    "    \"\"\"\n",
    "    alpha = np.exp(-1.0 / tau_samples)\n",
    "    y = np.zeros_like(wf, dtype=np.float64)\n",
    "    prev_x = wf[0]\n",
    "    prev_y = 0.0\n",
    "    for i in range(1, len(wf)):\n",
    "        x = float(wf[i])\n",
    "        y_i = x - prev_x + alpha * prev_y\n",
    "        y[i] = y_i\n",
    "        prev_x = x\n",
    "        prev_y = y_i\n",
    "    return y\n",
    "\n",
    "\n",
    "def compute_tfr_single(wf, tp0, tail_offset=200, tail_len=600):\n",
    "    \"\"\"\n",
    "    Tail Flattening Ratio (TFR) = std(tail_raw) / std(tail_pz)\n",
    "    tail is taken starting at tp0 + tail_offset, for length tail_len (or to end).\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    start = int(tp0) + tail_offset\n",
    "    if start >= n - 10:  # not enough tail\n",
    "        return np.nan\n",
    "\n",
    "    end = min(n, start + tail_len)\n",
    "    tail_raw = wf[start:end].astype(np.float64)\n",
    "\n",
    "    wf_pz = pole_zero_correct(wf)\n",
    "    tail_pz = wf_pz[start:end]\n",
    "\n",
    "    std_raw = np.std(tail_raw)\n",
    "    std_pz = np.std(tail_pz)\n",
    "\n",
    "    if std_pz <= 0:\n",
    "        return np.nan\n",
    "    return float(std_raw / std_pz)\n",
    "\n",
    "\n",
    "def smooth_gaussian(x, sigma=2.0):\n",
    "    \"\"\"Simple 1D Gaussian smoothing using convolution.\"\"\"\n",
    "    if sigma <= 0:\n",
    "        return x.astype(np.float64)\n",
    "    radius = int(3 * sigma)\n",
    "    idx = np.arange(-radius, radius + 1, dtype=np.float64)\n",
    "    kernel = np.exp(-0.5 * (idx / sigma) ** 2)\n",
    "    kernel /= kernel.sum()\n",
    "    padded = np.pad(x, radius, mode=\"edge\")\n",
    "    conv = np.convolve(padded, kernel, mode=\"same\")\n",
    "    return conv[radius:-radius]\n",
    "\n",
    "\n",
    "def compute_peak_count_single(\n",
    "    wf,\n",
    "    tp0,\n",
    "    window_after_tp0=400,\n",
    "    grad_threshold_frac=0.05,\n",
    "    min_separation=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Peak Count on the gradient:\n",
    "    - baseline-subtract and normalize waveform\n",
    "    - smooth\n",
    "    - compute gradient\n",
    "    - count local maxima above grad_threshold\n",
    "    \"\"\"\n",
    "    n = len(wf)\n",
    "    tp0 = int(tp0)\n",
    "\n",
    "    # Baseline: first 200 samples (guard against short)\n",
    "    base_end = min(200, n)\n",
    "    baseline = float(np.mean(wf[:base_end]))\n",
    "    wf_bs = wf - baseline\n",
    "\n",
    "    # Normalize by global max to make threshold comparable across events\n",
    "    max_val = np.max(np.abs(wf_bs))\n",
    "    if max_val <= 0:\n",
    "        return 0\n",
    "    wf_norm = wf_bs / max_val\n",
    "\n",
    "    # Only look near and after tp0\n",
    "    start = max(tp0 - 10, 0)\n",
    "    end = min(tp0 + window_after_tp0, n)\n",
    "    segment = wf_norm[start:end]\n",
    "\n",
    "    # Smooth and gradient\n",
    "    seg_smooth = smooth_gaussian(segment, sigma=2.0)\n",
    "    grad = np.gradient(seg_smooth)\n",
    "\n",
    "    # Threshold relative to max gradient\n",
    "    gmax = np.max(np.abs(grad))\n",
    "    if gmax <= 0:\n",
    "        return 0\n",
    "    threshold = grad_threshold_frac * gmax\n",
    "\n",
    "    # Count local maxima above threshold with minimum separation\n",
    "    count = 0\n",
    "    last_peak_idx = -min_separation - 1\n",
    "    for i in range(1, len(grad) - 1):\n",
    "        if grad[i] > grad[i - 1] and grad[i] > grad[i + 1] and grad[i] >= threshold:\n",
    "            if i - last_peak_idx >= min_separation:\n",
    "                count += 1\n",
    "                last_peak_idx = i\n",
    "\n",
    "    return int(count)\n",
    "\n",
    "\n",
    "def compute_gradient_baseline_noise_single(wf, baseline_region=(0, 200)):\n",
    "    \"\"\"\n",
    "    Gradient Baseline Noise = RMS of gradient in a pre-rise baseline window.\n",
    "    \"\"\"\n",
    "    start, end = baseline_region\n",
    "    start = max(start, 0)\n",
    "    end = min(end, len(wf))\n",
    "    if end - start < 5:\n",
    "        return np.nan\n",
    "\n",
    "    segment = wf[start:end].astype(np.float64)\n",
    "    grad = np.gradient(segment)\n",
    "    return float(np.sqrt(np.mean(grad ** 2)))\n",
    "\n",
    "\n",
    "def compute_band_power_ratio_single(\n",
    "    wf,\n",
    "    fs=100e6,\n",
    "    low_band=(0.1e6, 1e6),\n",
    "    high_band=(1e6, 10e6),\n",
    "):\n",
    "    \"\"\"\n",
    "    Band Power Ratio (BPR) = power_high / power_low using FFT of the waveform.\n",
    "    \"\"\"\n",
    "    x = wf.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "\n",
    "    # Real FFT\n",
    "    fft_vals = np.fft.rfft(x)\n",
    "    psd = np.abs(fft_vals) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(x), d=1.0 / fs)\n",
    "\n",
    "    low_mask = (freqs >= low_band[0]) & (freqs < low_band[1])\n",
    "    high_mask = (freqs >= high_band[0]) & (freqs < high_band[1])\n",
    "\n",
    "    power_low = psd[low_mask].sum()\n",
    "    power_high = psd[high_mask].sum()\n",
    "\n",
    "    if power_low <= 0:\n",
    "        return np.nan\n",
    "    return float(power_high / power_low)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Per-file processing\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def process_hdf5_file(h5_path, out_dir):\n",
    "    \"\"\"\n",
    "    Read one HDF5 file, compute all 5 parameters, and write a CSV with:\n",
    "      id, file, tdrift99, tfr, peak_count, gbn, bpr\n",
    "    \"\"\"\n",
    "    print(f\"Processing {h5_path}...\")\n",
    "    basename = os.path.basename(h5_path)\n",
    "\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        waveforms = f[\"raw_waveform\"][:]          # shape: (N, 3800)\n",
    "        tp0 = f[\"tp0\"][:]                         # shape: (N,)\n",
    "        ids = f[\"id\"][:]                          # shape: (N,)\n",
    "\n",
    "    n_events = waveforms.shape[0]\n",
    "    print(f\"  Found {n_events} waveforms.\")\n",
    "\n",
    "    tdrift_list = []\n",
    "    tfr_list = []\n",
    "    peak_count_list = []\n",
    "    gbn_list = []\n",
    "    bpr_list = []\n",
    "\n",
    "    for i in range(n_events):\n",
    "        wf = waveforms[i]\n",
    "\n",
    "        t0 = tp0[i]\n",
    "        tdrift_list.append(compute_tdrift99_single(wf, t0))\n",
    "        tfr_list.append(compute_tfr_single(wf, t0))\n",
    "        peak_count_list.append(compute_peak_count_single(wf, t0))\n",
    "        gbn_list.append(compute_gradient_baseline_noise_single(wf))\n",
    "        bpr_list.append(compute_band_power_ratio_single(wf))\n",
    "\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"    Processed {i + 1}/{n_events} events...\")\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": ids,\n",
    "            \"file\": basename,\n",
    "            \"tdrift99\": tdrift_list,\n",
    "            \"tfr\": tfr_list,\n",
    "            \"peak_count\": peak_count_list,\n",
    "            \"gbn\": gbn_list,\n",
    "            \"bpr\": bpr_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_name = os.path.splitext(basename)[0] + \"_myparams.csv\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"  Saved CSV to {out_path}\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Main: loop over all train/test files\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Notebook-safe data directory\n",
    "    DATA_DIR = os.path.abspath(\"data\")\n",
    "\n",
    "    TRAIN_PATTERN = os.path.join(DATA_DIR, \"MJD_Train*.hdf5\")\n",
    "    TEST_PATTERN = os.path.join(DATA_DIR, \"MJD_Test*.hdf5\")\n",
    "\n",
    "    OUT_DIR_TRAIN = os.path.join(DATA_DIR, \"params_train\")\n",
    "    OUT_DIR_TEST = os.path.join(DATA_DIR, \"params_test\")\n",
    "\n",
    "    train_files = sorted(glob.glob(TRAIN_PATTERN))\n",
    "    test_files = sorted(glob.glob(TEST_PATTERN))\n",
    "\n",
    "    print(\"Train files:\", train_files)\n",
    "    print(\"Test files:\", test_files)\n",
    "\n",
    "    for path in train_files:\n",
    "        process_hdf5_file(path, OUT_DIR_TRAIN)\n",
    "\n",
    "    for path in test_files:\n",
    "        process_hdf5_file(path, OUT_DIR_TEST)\n",
    "    process_hdf5_file(\"../../data/MJD_Train_15.hdf5\", \"../../data/params_train\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa0c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments\n",
      "data exists? False\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCWD =\u001b[39m\u001b[33m\"\u001b[39m, os.getcwd())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdata exists?\u001b[39m\u001b[33m\"\u001b[39m, os.path.exists(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfiles in data:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD =\", os.getcwd())\n",
    "print(\"data exists?\", os.path.exists(\"data\"))\n",
    "print(\"files in data:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181ecc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = (Path.cwd() / \"data\").resolve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bf96f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_PATTERN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTRAIN_PATTERN =\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mTRAIN_PATTERN\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMatches:\u001b[39m\u001b[33m\"\u001b[39m, glob.glob(TRAIN_PATTERN))\n",
      "\u001b[31mNameError\u001b[39m: name 'TRAIN_PATTERN' is not defined"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "print(\"TRAIN_PATTERN =\", TRAIN_PATTERN)\n",
    "print(\"Matches:\", glob.glob(TRAIN_PATTERN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419403a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments\n",
      "DATA_DIR: /Users/prithvikochhar/Documents/GitHub/Majorana-Neutrino-Hunt/src/experiments/data\n",
      "Train files found: []\n",
      "Test files found: []\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "DATA_DIR = (Path.cwd() / \"data\").resolve()   # assumes you opened VSCode at repo root\n",
    "TRAIN_PATTERN = str(DATA_DIR / \"MJD_Train*.hdf5\")\n",
    "TEST_PATTERN  = str(DATA_DIR / \"MJD_Test*.hdf5\")\n",
    "\n",
    "train_files = sorted(glob.glob(TRAIN_PATTERN))\n",
    "test_files  = sorted(glob.glob(TEST_PATTERN))\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Train files found:\", train_files)\n",
    "print(\"Test files found:\", test_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25478b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
