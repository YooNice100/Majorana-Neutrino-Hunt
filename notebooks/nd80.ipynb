{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b682cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Python path: c:\\Users\\YooNi\\OneDrive\\Desktop\\Majorana-Neutrino-Hunt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Get the project root (one level above notebooks/)\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Added to Python path:\", project_root)\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from src.parameters.tail_features import compute_LQ80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12a552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = \"../data/old/MJD_Train_0.hdf5\"\n",
    "\n",
    "# with h5py.File(train_file, \"r\") as f:\n",
    "#     waveforms = np.array(f[\"raw_waveform\"])\n",
    "#     ids = np.array(f[\"id\"])\n",
    "\n",
    "# print(\"Loaded\", len(waveforms), \"waveforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7ce1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_baseline(y, n_samples=200):\n",
    "    \"\"\"\n",
    "    Returns baseline (mean, std) from the first n_samples.\n",
    "    \"\"\"\n",
    "    y0 = np.asarray(y, dtype=float)[:n_samples]\n",
    "    return float(np.mean(y0)), float(np.std(y0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e36c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ND80(waveform, n_pre=200):\n",
    "    \"\"\"\n",
    "    ND80: Normalized maximum dip below the 80 percent amplitude level\n",
    "    between the 80 percent crossing and the peak.\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.asarray(waveform, dtype=float)\n",
    "\n",
    "    baseline, _ = estimate_baseline(y, n_samples=n_pre)\n",
    "    peak_idx = int(np.argmax(y))\n",
    "    peak_val = float(y[peak_idx])\n",
    "    amp = peak_val - baseline\n",
    "\n",
    "    if amp <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    level80 = baseline + 0.80 * amp\n",
    "\n",
    "    above = np.where(y >= level80)[0]\n",
    "    if len(above) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    i80 = int(above[0])\n",
    "\n",
    "    if i80 >= peak_idx:\n",
    "        return 0.0\n",
    "\n",
    "    seg = y[i80: peak_idx + 1]\n",
    "\n",
    "    depth_vec = level80 - seg\n",
    "    depth_vec[depth_vec < 0] = 0.0\n",
    "\n",
    "    depth_abs = float(np.max(depth_vec))\n",
    "    depth_norm = depth_abs / amp if amp > 0 else np.nan\n",
    "\n",
    "    return depth_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a012d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"finalcsveunice\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69feba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ND80 train\n",
    "all_train_ids_nd80 = []\n",
    "all_ND80_train = []\n",
    "\n",
    "for train_idx in range(16):\n",
    "    train_file = f\"../data/old/MJD_Train_{train_idx}.hdf5\"\n",
    "    if not os.path.exists(train_file):\n",
    "        print(f\"Skipping missing file: {train_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nLoading {train_file}\")\n",
    "\n",
    "    with h5py.File(train_file, \"r\") as f:\n",
    "        waveforms_train = np.array(f[\"raw_waveform\"])\n",
    "        ids_train = np.array(f[\"id\"])\n",
    "\n",
    "    print(f\"  Waveforms: {len(waveforms_train)}\")\n",
    "\n",
    "    for i, wf in enumerate(waveforms_train):\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"    ND80 Train_{train_idx} {i}/{len(waveforms_train)}\")\n",
    "\n",
    "        all_ND80_train.append(compute_ND80(wf))\n",
    "        all_train_ids_nd80.append(f\"{ids_train[i]}_train_{train_idx}\")\n",
    "\n",
    "all_ND80_train = np.array(all_ND80_train, dtype=float)\n",
    "all_ND80_train[~np.isfinite(all_ND80_train)] = np.nan\n",
    "\n",
    "df_nd80_train = pd.DataFrame({\n",
    "    \"id\": all_train_ids_nd80,\n",
    "    \"ND80\": all_ND80_train\n",
    "})\n",
    "\n",
    "output_path_nd80_train = os.path.join(OUTPUT_DIR, \"ND80_train_all.csv\")\n",
    "df_nd80_train.to_csv(output_path_nd80_train, index=False)\n",
    "\n",
    "print(\"\\nSaved combined ND80 TRAIN CSV to:\", output_path_nd80_train)\n",
    "print(df_nd80_train.head())\n",
    "print(df_nd80_train[\"ND80\"].describe())\n",
    "print(\"NaNs:\", df_nd80_train[\"ND80\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbf058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa9c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ND80 test\n",
    "all_test_ids_nd80 = []\n",
    "all_ND80_test = []\n",
    "\n",
    "for test_idx in range(6):\n",
    "    test_file = f\"../data/old/MJD_Test_{test_idx}.hdf5\"\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"Skipping missing file: {test_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nLoading {test_file}\")\n",
    "\n",
    "    with h5py.File(test_file, \"r\") as f:\n",
    "        waveforms_test = np.array(f[\"raw_waveform\"])\n",
    "        ids_test = np.array(f[\"id\"])\n",
    "\n",
    "    print(f\"  Waveforms: {len(waveforms_test)}\")\n",
    "\n",
    "    for i, wf in enumerate(waveforms_test):\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"    ND80 Test_{test_idx} {i}/{len(waveforms_test)}\")\n",
    "\n",
    "        all_ND80_test.append(compute_ND80(wf))\n",
    "        all_test_ids_nd80.append(f\"{ids_test[i]}_test_{test_idx}\")\n",
    "\n",
    "all_ND80_test = np.array(all_ND80_test, dtype=float)\n",
    "all_ND80_test[~np.isfinite(all_ND80_test)] = np.nan\n",
    "\n",
    "df_nd80_test = pd.DataFrame({\n",
    "    \"id\": all_test_ids_nd80,\n",
    "    \"ND80\": all_ND80_test\n",
    "})\n",
    "\n",
    "output_path_nd80_test = os.path.join(OUTPUT_DIR, \"ND80_test_all.csv\")\n",
    "df_nd80_test.to_csv(output_path_nd80_test, index=False)\n",
    "\n",
    "print(\"\\nSaved combined ND80 TEST CSV to:\", output_path_nd80_test)\n",
    "print(df_nd80_test.head())\n",
    "print(df_nd80_test[\"ND80\"].describe())\n",
    "print(\"NaNs:\", df_nd80_test[\"ND80\"].isna().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
